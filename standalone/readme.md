# AnythingLLM with Ollama Integration

## Overview
This repository contains configuration for running AnythingLLM with Ollama as the LLM provider in a containerized environment.

## Setup Instructions

### Step 1: Configure Docker Compose
Update your `docker-compose.yml` file to include Ollama as a service:

```yml
services:
  anythingllm:
    image: mintplexlabs/anythingllm
    container_name: anythingllm
    ports:
      - "3001:3001"
    cap_add:
      - SYS_ADMIN
    environment:
      - STORAGE_DIR=/app/server/storage
      - JWT_SECRET="make this a large list of random numbers and letters 20+"
      - LLM_PROVIDER=ollama
      - OLLAMA_BASE_PATH=http://ollama:11434
      - OLLAMA_MODEL_PREF=llama2
      - OLLAMA_MODEL_TOKEN_LIMIT=4096
      - EMBEDDING_ENGINE=ollama
      - EMBEDDING_BASE_PATH=http://ollama:11434
      - EMBEDDING_MODEL_PREF=nomic-embed-text:latest
      - EMBEDDING_MODEL_MAX_CHUNK_LENGTH=8192
      - VECTOR_DB=lancedb
      - WHISPER_PROVIDER=local
      - TTS_PROVIDER=native
      - PASSWORDMINCHAR=8
    volumes:
      - anythingllm_storage:/app/server/storage
    restart: always
    depends_on:
      - ollama

  ollama:
    # Generated by Copilot
    image: ollama/ollama:latest
    container_name: ollama
    ports:
      - "11434:11434"
    volumes:
      - ollama_data:/root/.ollama
    restart: always

volumes:
  anythingllm_storage:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: C:/Projects/VTRepos/hackathon/document-storage/docker
  ollama_data:
    driver: local
```

### Step 2: Start the Services
Start both containers using:

```bash
docker-compose up -d
```

### Step 3: Pull Required Models
After the containers are running, pull the necessary models:

```bash
docker exec -it ollama ollama pull llama2; docker exec -it ollama ollama pull nomic-embed-text:latest
```

### Step 4: Access AnythingLLM
Once everything is set up, you can access AnythingLLM at:

http://localhost:3001

## Key Changes

1. Added Ollama as a separate service
2. Changed OLLAMA_BASE_PATH from http://127.0.0.1:11434 to http://ollama:11434
3. Changed EMBEDDING_BASE_PATH from http://127.0.0.1:11434 to http://ollama:11434
4. Added dependency between services
5. Created a persistent volume for Ollama data

## Troubleshooting

If you encounter issues:

1. Check container logs: `docker logs ollama` or `docker logs anythingllm`
2. Verify the models were successfully pulled: `docker exec -it ollama ollama list`
3. Ensure port 11434 is not being used by another application

# Generated by Copilot
```